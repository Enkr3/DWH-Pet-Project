# DWH-Pet-Project-
Аналитическая витрина розничных данных (Greenplum)

**Описание проекта** <br>
Проект представляет собой реализацию аналитической витрины данных для розничной сети на базе СУБД Greenplum.
Витрина предназначена для формирования дневной и месячной отчётности по продажам, купонам и трафику магазинов.
Проект выполнен в рамках учебного курса и оформлен как pet-project с упором на архитектуру DWH, ELT-процессы и оптимизацию SQL-запросов.

**Архитектура данных**<br>
Используется разделение данных на:
- Таблицы фактов: продажи, чеки, трафик
- Таблицы-справочники: магазины, акции, типы акций, купоны
![telegram-cloud-photo-size-2-5199547955151573210-y](https://github.com/user-attachments/assets/1ecf2f64-254f-4831-8c9b-4f5f9702776b)

Для хранения данных применяются разные форматы:
- AO Column — для аналитических сценариев и агрегаций
- Партиционирование по дате — для таблиц фактов

Загрузка данных (ELT)
В проекте реализованы несколько типов загрузок:

FULL load
- Полная перезапись справочников
- Используется TRUNCATE + INSERT
- Загрузка данных через gpfdist

DELTA PARTITION load
- Инкрементальная загрузка таблиц фактов
- Чтение данных через PXF (JDBC)
- Загрузка данных во временную таблицу
- Атомарная подмена партиций с помощью EXCHANGE PARTITION

Такой подход обеспечивает:
- отказоустойчивость загрузок
- минимальные блокировки
- высокую производительность

**Витрина и метрики**<br>
На базе загруженных данных формируется витрина со следующими показателями:
- оборот
- оборот с учётом скидок
- количество чеков
- средний чек
- конверсия магазина
- доля товаров со скидкой
- средняя выручка на посетителя
Отчёт формируется за произвольный период.

**Оптимизация запросов**<br>
В проекте применены:
- фильтрация данных до JOIN
- анализ EXPLAIN ANALYZE
- использование partition pruning
- выбор оптимальных типов JOIN
- минимизация перераспределений данных (Motion)
В результате время выполнения аналитических запросов сокращено на 60–70%.

**Оркестрация**<br>
Процессы загрузки и сборки витрины автоматизированы с помощью Apache Airflow:
- FULL загрузки справочников
- DELTA загрузки фактов
- сборка аналитической витрины
![telegram-cloud-photo-size-2-5199547955151573212-w](https://github.com/user-attachments/assets/aae8bb66-62eb-4f0a-bc4d-c169b7174cad)


**Используемые технологии**<br>
- Greenplum
- Clickhouse
- SQL (DDL, DML, UDF, EXPLAIN ANALYZE)
- PXF, gpfdist
- Apache Airflow
- Apache Superset

**Цель проекта**<br>
Закрепить практические навыки:
- проектирования аналитических витрин
- работы с MPP-СУБД
- построения ELT-процессов
- оптимизации SQL-запросов
- анализа планов выполнения

**Визуализация отчета в SuperSet**<br>
<img width="3014" height="1664" alt="image" src="https://github.com/user-attachments/assets/f0072150-dea4-41be-815c-92bd420593d2" />
